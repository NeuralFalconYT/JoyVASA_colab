{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NeuralFalconYT/JoyVASA_colab/refs/heads/main/colab.png\" width=\"800\" height=\"150\">\n",
        "\n"
      ],
      "metadata": {
        "id": "B6OV6vbVRHAZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "QSMiL5Y9LuJL"
      },
      "outputs": [],
      "source": [
        "#@title Install and Restart Session\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/jdh-algo/JoyVASA\n",
        "!pip install tyro gradio onnx onnxruntime onnxruntime-gpu pykalman colorama\n",
        "%cd /content/JoyVASA/src/utils/dependencies/XPose/models/UniPose/ops\n",
        "!python setup.py build install\n",
        "%cd /content/JoyVASA\n",
        "!wget https://raw.githubusercontent.com/NeuralFalconYT/JoyVASA_colab/refs/heads/main/download_model.py\n",
        "!python download_model.py\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import time\n",
        "time.sleep(5)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Gradio App\n",
        "%cd /content/JoyVASA\n",
        "import os\n",
        "import tyro\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "import os.path as osp\n",
        "import platform\n",
        "from src.utils.helper import load_description\n",
        "from src.gradio_pipeline import GradioPipeline, GradioPipelineAnimal\n",
        "from src.config.crop_config import CropConfig\n",
        "from src.config.argument_config import ArgumentConfig\n",
        "from src.config.base_config import make_abs_path\n",
        "from src.config.inference_config import InferenceConfig\n",
        "import argparse\n",
        "\n",
        "\n",
        "if platform.system() == \"Windows\":\n",
        "    import pathlib\n",
        "    temp = pathlib.PosixPath\n",
        "    pathlib.PosixPath = pathlib.WindowsPath\n",
        "\n",
        "def partial_fields(target_class, kwargs):\n",
        "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
        "\n",
        "def fast_check_ffmpeg():\n",
        "    try:\n",
        "        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "ffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\n",
        "if osp.exists(ffmpeg_dir):\n",
        "    os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n",
        "if not fast_check_ffmpeg():\n",
        "    raise ImportError(\n",
        "        \"FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html\"\n",
        "    )\n",
        "tyro.extras.set_accent_color(\"bright_cyan\")\n",
        "dummy_image=make_abs_path('../../assets/examples/imgs/joyvasa_001.png')\n",
        "dummy_audio=make_abs_path('../../assets/examples/audios/joyvasa_001.wav')\n",
        "output_video_save_path=make_abs_path('../../animations/')\n",
        "# Define arguments as a string (similar to command-line arguments)\n",
        "args_string = f'--reference {dummy_image} --audio {dummy_audio} --output_dir {output_video_save_path} --animation_mode human'\n",
        "\n",
        "# Use tyro.cli to parse the arguments string\n",
        "args = tyro.cli(ArgumentConfig, args=args_string.split())\n",
        "\n",
        "# args = argparse.ArgumentParser()\n",
        "# specify configs for inference\n",
        "inference_cfg = partial_fields(InferenceConfig, args.__dict__)  # use attribute of args to initial InferenceConfig\n",
        "crop_cfg = partial_fields(CropConfig, args.__dict__)  # use attribute of args to initial CropConfig\n",
        "gradio_temp_dir=\"./temp\"\n",
        "os.environ[\"GRADIO_TEMP_DIR\"] = gradio_temp_dir\n",
        "os.makedirs(gradio_temp_dir, exist_ok=True)\n",
        "############# Functions #################\n",
        "# if args.gradio_temp_dir not in (None, ''):\n",
        "#     os.environ[\"GRADIO_TEMP_DIR\"] = args.gradio_temp_dir\n",
        "#     os.makedirs(args.gradio_temp_dir, exist_ok=True)\n",
        "\n",
        "gradio_pipeline_human = GradioPipeline(\n",
        "    inference_cfg=inference_cfg,\n",
        "    crop_cfg=crop_cfg,\n",
        "    args=args\n",
        ")\n",
        "gradio_pipeline_animal = GradioPipelineAnimal(\n",
        "    inference_cfg=inference_cfg,\n",
        "    crop_cfg=crop_cfg,\n",
        "    args=args\n",
        ")\n",
        "\n",
        "def gpu_wrapped_execute_a2v(*args, **kwargs):\n",
        "    # print(\"args: \", args, args[5])\n",
        "    if args[5] == \"animal\":\n",
        "        return gradio_pipeline_animal.execute_a2v(*args, **kwargs)\n",
        "    else:\n",
        "        return gradio_pipeline_human.execute_a2v(*args, **kwargs)\n",
        "\n",
        "\n",
        "################# GUI ################\n",
        "title_md = \"assets/gradio/gradio_title.md\"\n",
        "example_reference_dir = \"assets/examples/imgs\"\n",
        "example_audio_dir = \"assets/examples/audios\"\n",
        "data_examples_a2v = [\n",
        "    [osp.join(example_reference_dir, \"joyvasa_001.png\"), osp.join(example_audio_dir, \"joyvasa_001.wav\"), \"animal\", False, 4.0],\n",
        "    [osp.join(example_reference_dir, \"joyvasa_002.png\"), osp.join(example_audio_dir, \"joyvasa_002.wav\"), \"animal\", False, 4.0],\n",
        "    [osp.join(example_reference_dir, \"joyvasa_003.png\"), osp.join(example_audio_dir, \"joyvasa_003.wav\"), \"human\", False, 4.0],\n",
        "    [osp.join(example_reference_dir, \"joyvasa_004.png\"), osp.join(example_audio_dir, \"joyvasa_004.wav\"), \"human\", False, 4.0],\n",
        "    [osp.join(example_reference_dir, \"joyvasa_005.png\"), osp.join(example_audio_dir, \"joyvasa_005.wav\"), \"human\", False, 4.0],\n",
        "    [osp.join(example_reference_dir, \"joyvasa_006.png\"), osp.join(example_audio_dir, \"joyvasa_006.wav\"), \"human\", False, 4.0],\n",
        "]\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(font=[gr.themes.GoogleFont(\"Plus Jakarta Sans\")])) as demo:\n",
        "    gr.HTML(load_description(title_md))\n",
        "\n",
        "    # Inputs & Outputs\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_upload.md\"))\n",
        "    with gr.Row():\n",
        "        with gr.Accordion(open=True, label=\"üñºÔ∏è Reference Image\"):\n",
        "            input_image = gr.Image(type=\"filepath\", width=512, label=\"Reference Image\")\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [osp.join(example_reference_dir, \"joyvasa_001.png\")],\n",
        "                    [osp.join(example_reference_dir, \"joyvasa_002.png\")],\n",
        "                    [osp.join(example_reference_dir, \"joyvasa_003.png\")],\n",
        "                    [osp.join(example_reference_dir, \"joyvasa_004.png\")],\n",
        "                    [osp.join(example_reference_dir, \"joyvasa_005.png\")],\n",
        "                    [osp.join(example_reference_dir, \"joyvasa_006.png\")],\n",
        "                ],\n",
        "                inputs=[input_image],\n",
        "                cache_examples=False,\n",
        "            )\n",
        "        with gr.Accordion(open=True, label=\"üéµ Input Audio\"):\n",
        "            input_audio = gr.Audio(type=\"filepath\", label=\"Input Audio\")\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [osp.join(example_audio_dir, \"joyvasa_001.wav\")],\n",
        "                    [osp.join(example_audio_dir, \"joyvasa_002.wav\")],\n",
        "                    [osp.join(example_audio_dir, \"joyvasa_003.wav\")],\n",
        "                    [osp.join(example_audio_dir, \"joyvasa_004.wav\")],\n",
        "                    [osp.join(example_audio_dir, \"joyvasa_005.wav\")],\n",
        "                    [osp.join(example_audio_dir, \"joyvasa_006.wav\")],\n",
        "                ],\n",
        "                inputs=[input_audio],\n",
        "                cache_examples=False,\n",
        "            )\n",
        "        with gr.Accordion(open=True, label=\"üé¨ Output Video\",):\n",
        "            output_video = gr.Video(autoplay=False, interactive=False, width=512)\n",
        "\n",
        "    # Configs\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_configuration.md\"))\n",
        "\n",
        "    with gr.Column():\n",
        "        with gr.Accordion(open=True, label=\"Key Animation Options\"):\n",
        "            with gr.Row():\n",
        "                animation_mode =gr.Radio(['human', 'animal'], value=\"human\", label=\"Animation Mode\")\n",
        "                flag_do_crop_input = gr.Checkbox(value=True, label=\"do crop (image)\")\n",
        "                cfg_scale = gr.Number(value=4.0, label=\"cfg_scale\", minimum=0.0, maximum=10.0, step=0.5)\n",
        "        with gr.Accordion(open=False, label=\"Optional Animation Options\"):\n",
        "            with gr.Row():\n",
        "                driving_option_input = gr.Radio(['expression-friendly', 'pose-friendly'], value=\"expression-friendly\", label=\"driving option\")\n",
        "                driving_multiplier = gr.Number(value=1.0, label=\"driving multiplier\", minimum=0.0, maximum=2.0, step=0.02)\n",
        "            with gr.Row():\n",
        "                flag_normalize_lip = gr.Checkbox(value=True, label=\"normalize lip\")\n",
        "                flag_relative_motion = gr.Checkbox(value=True, label=\"relative motion\")\n",
        "                flag_remap_input = gr.Checkbox(value=True, label=\"paste-back\")\n",
        "                flag_stitching_input = gr.Checkbox(value=True, label=\"stitching\")\n",
        "        with gr.Accordion(open=False, label=\"Optional Options for Reference Image\"):\n",
        "            with gr.Row():\n",
        "                scale = gr.Number(value=2.3, label=\"image crop scale\", minimum=1.8, maximum=4.0, step=0.05)\n",
        "                vx_ratio = gr.Number(value=0.0, label=\"image crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "                vy_ratio = gr.Number(value=-0.125, label=\"image crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "\n",
        "    # Generate\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_generate.md\"))\n",
        "    with gr.Row():\n",
        "        process_button_generate = gr.Button(\"üöÄ Generate\", variant=\"primary\")\n",
        "\n",
        "    # Examples\n",
        "    gr.Examples(\n",
        "        examples=data_examples_a2v,\n",
        "        inputs=[input_image,\n",
        "                input_audio,\n",
        "                animation_mode,\n",
        "                flag_do_crop_input,\n",
        "                cfg_scale,\n",
        "                ],\n",
        "        outputs=[output_video],\n",
        "        cache_examples=False\n",
        "    )\n",
        "\n",
        "    # Binding Functions for Buttons\n",
        "    generation_func = gpu_wrapped_execute_a2v\n",
        "    process_button_generate.click(\n",
        "        fn=generation_func,\n",
        "        inputs=[\n",
        "            input_image,\n",
        "            input_audio,\n",
        "            flag_normalize_lip,\n",
        "            flag_relative_motion,\n",
        "            driving_multiplier,\n",
        "            animation_mode,\n",
        "            driving_option_input,\n",
        "            flag_do_crop_input,\n",
        "            scale,\n",
        "            vx_ratio,\n",
        "            vy_ratio,\n",
        "            flag_stitching_input,\n",
        "            flag_remap_input,\n",
        "            cfg_scale,\n",
        "        ],\n",
        "        outputs=[\n",
        "            output_video,\n",
        "        ],\n",
        "        show_progress=True\n",
        "    )\n",
        "demo.queue().launch(allowed_paths=[gradio_temp_dir],debug=True,share=True)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jvvflwrJN4NU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}